{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a0a9d53",
   "metadata": {},
   "source": [
    "# NoiseDiffusion Colab Notebook (Colab Pro / A100-ready)\n",
    "**Purpose:** reproduce and experiment with *Noise Diffusion* for improving semantic faithfulness in text-to-image generation.\n",
    "\n",
    "**What this notebook provides**\n",
    "- Environment setup (installs optimized for Colab Pro / A100)\n",
    "- Lightweight **CLIP-MonteCarlo** demo (fast proxy; good for pilots)\n",
    "- **NoiseDiffusion** implementation (latent updates per paper) — more expensive but faithful\n",
    "- Utilities: GPU detection, HF token input, CLIP scoring, saving outputs\n",
    "\n",
    "**Important**\n",
    "- Set *Runtime → Change runtime type* to **GPU** (preferably A100).  \n",
    "- Use Colab Pro for larger jobs; for heavy reproduction (paper-scale) prefer a >=48GB GPU.  \n",
    "- This notebook will prompt for your Hugging Face token when needed. Do **not** share the token publicly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acec051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# This installation is tuned for Colab. It may take a few minutes.\n",
    "import sys\n",
    "print(\"Python\", sys.version)\n",
    "# Install packages\n",
    "!pip install -q --upgrade pip\n",
    "# core libs\n",
    "!pip install -q diffusers[torch]==0.19.1 transformers accelerate safetensors ftfy\n",
    "# Use transformers' CLIP implementation instead of openai/CLIP to avoid building from source\n",
    "!pip install -q transformers[torch]\n",
    "# optional performance libs (xformers can speed up attention if available)\n",
    "try:\n",
    "    get_ipython().system_raw('pip install -q xformers==0.0.20')\n",
    "except Exception as e:\n",
    "    print('xformers install skipped or failed (optional):', e)\n",
    "print('Install finished. Restart runtime if required by Colab.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c1ce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU detection and recommended default parameters\n",
    "import torch, subprocess, json\n",
    "def gpu_info():\n",
    "    try:\n",
    "        out = subprocess.check_output(['nvidia-smi', '--query-gpu=name,memory.total,driver_version', '--format=csv,noheader,nounits']).decode().strip()\n",
    "        name, mem, drv = [x.strip() for x in out.split(',')]\n",
    "        return name, int(mem), drv\n",
    "    except Exception as e:\n",
    "        return None, None, None\n",
    "\n",
    "gpu_name, gpu_mem, driver = gpu_info()\n",
    "print('GPU:', gpu_name, 'VRAM(MB):', gpu_mem, 'driver:', driver)\n",
    "# Set safe defaults (you can override later)\n",
    "if gpu_name and 'A100' in gpu_name:\n",
    "    DEFAULT_IMAGE_SIZE = 512\n",
    "    DEFAULT_STEPS = 20\n",
    "    DEFAULT_NUM_CANDS = 8\n",
    "    DEFAULT_NUM_ITERS = 3\n",
    "else:\n",
    "    # T4 or others\n",
    "    DEFAULT_IMAGE_SIZE = 384\n",
    "    DEFAULT_STEPS = 20\n",
    "    DEFAULT_NUM_CANDS = 4\n",
    "    DEFAULT_NUM_ITERS = 2\n",
    "\n",
    "print('Defaults set -> image_size:', DEFAULT_IMAGE_SIZE, 'steps:', DEFAULT_STEPS,\n",
    "      'num_candidates:', DEFAULT_NUM_CANDS, 'num_iters:', DEFAULT_NUM_ITERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8493f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face token (you will be prompted)\n",
    "from getpass import getpass\n",
    "import os\n",
    "hf_token = os.environ.get('HF_TOKEN', None)\n",
    "if not hf_token:\n",
    "    hf_token = getpass('Paste your Hugging Face token (with access to SD models): ')\n",
    "    os.environ['HF_TOKEN'] = hf_token\n",
    "print('HF token set in env (session only).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f241817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Stable Diffusion pipeline and CLIP scorer (transformers)\n",
    "import os, torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "MODEL_ID = 'runwayml/stable-diffusion-v1-5'\n",
    "hf_token = os.environ.get('HF_TOKEN') or ''\n",
    "\n",
    "print('Loading Stable Diffusion model (this will download weights)...')\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16 if device=='cuda' else torch.float32,\n",
    "    use_auth_token=hf_token\n",
    ").to(device)\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_attention_slicing()\n",
    "try:\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "except Exception as e:\n",
    "    print('xformers not enabled or not installed:', e)\n",
    "\n",
    "print('Loading CLIP scorer (transformers)...')\n",
    "clip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32').to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "print('Models loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da8fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions: decode from seed or from explicit latent; CLIP scoring\n",
    "import torch, numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def decode_with_seed(prompt, seed, steps=DEFAULT_STEPS, image_size=DEFAULT_IMAGE_SIZE, guidance_scale=7.5):\n",
    "    gen = torch.Generator(device=device).manual_seed(int(seed))\n",
    "    out = pipe(prompt, height=image_size, width=image_size, num_inference_steps=steps, guidance_scale=guidance_scale, generator=gen)\n",
    "    return out.images[0]\n",
    "\n",
    "def clip_score(prompt, pil_image):\n",
    "    inputs = clip_processor(text=[prompt], images=pil_image, return_tensors='pt', padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        img_emb = clip_model.get_image_features(inputs['pixel_values'])\n",
    "        txt_emb = clip_model.get_text_features(inputs['input_ids'])\n",
    "        img_emb = img_emb / img_emb.norm(p=2, dim=-1, keepdim=True)\n",
    "        txt_emb = txt_emb / txt_emb.norm(p=2, dim=-1, keepdim=True)\n",
    "        sim = (img_emb @ txt_emb.T).cpu().numpy().item()\n",
    "    return float(sim)\n",
    "\n",
    "# Functions for latent handling\n",
    "def prepare_latent_from_seed(seed, image_size=DEFAULT_IMAGE_SIZE):\n",
    "    # sample random normal latent for z_T consistent with sd pipeline\n",
    "    # pipeline.vae.encode / prepare_latents methods vary across versions,\n",
    "    # here we use pipeline.prepare_latents convenience if available.\n",
    "    generator = torch.Generator(device=device).manual_seed(int(seed))\n",
    "    # num_channels_latents = pipe.unet.in_channels? Use typical latent shape for SD\n",
    "    batch = 1\n",
    "    latents = torch.randn((batch, pipe.unet.in_channels, image_size // 8, image_size // 8), generator=generator, device=device, dtype=torch.float16 if device=='cuda' else torch.float32)\n",
    "    return latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ee2e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP-MonteCarlo (fast proxy) demo\n",
    "import random, time\n",
    "def clip_monte_carlo(prompt, iterations=DEFAULT_NUM_ITERS, candidates=DEFAULT_NUM_CANDS, steps=DEFAULT_STEPS, image_size=DEFAULT_IMAGE_SIZE):\n",
    "    best_seed = random.randint(0,2**31-2)\n",
    "    best_img = decode_with_seed(prompt, best_seed, steps, image_size)\n",
    "    best_score = clip_score(prompt, best_img)\n",
    "    print(f'Init seed {best_seed}, CLIP score {best_score:.4f}')\n",
    "    for it in range(iterations):\n",
    "        t0 = time.time()\n",
    "        cand = []\n",
    "        for i in range(candidates):\n",
    "            s = random.randint(0,2**31-2)\n",
    "            img = decode_with_seed(prompt, s, steps, image_size)\n",
    "            sc = clip_score(prompt, img)\n",
    "            cand.append((sc, s, img))\n",
    "        sc_chosen, seed_chosen, img_chosen = max(cand, key=lambda x: x[0])\n",
    "        if sc_chosen > best_score:\n",
    "            best_score, best_seed, best_img = sc_chosen, seed_chosen, img_chosen\n",
    "            print(f'Iter {it+1}: improved to seed {best_seed}, CLIP {best_score:.4f} (time {time.time()-t0:.1f}s)')\n",
    "        else:\n",
    "            print(f'Iter {it+1}: no improvement (best {best_score:.4f}) (time {time.time()-t0:.1f}s)')\n",
    "    return best_img, best_seed, best_score\n",
    "\n",
    "# Quick run example\n",
    "prompt='A photorealistic red vintage car parked in front of a Victorian house'\n",
    "img, seed, score = clip_monte_carlo(prompt)\n",
    "display(img)\n",
    "print('Final seed', seed, 'score', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3b6dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NoiseDiffusion (latent update) implementation (expensive)\n",
    "# WARNING: This performs explicit latent updates and denoises from provided latents.\n",
    "# Tune num_iters, num_cands conservatively on Colab (start small).\n",
    "\n",
    "import math, time, torch\n",
    "\n",
    "def noise_diffusion_latent(prompt, num_iters=DEFAULT_NUM_ITERS, num_cands=DEFAULT_NUM_CANDS, steps=DEFAULT_STEPS, image_size=DEFAULT_IMAGE_SIZE, guidance_scale=7.5, gamma_rule='score'):\n",
    "    # Initialize latent z_T (random)\n",
    "    seed0 = random.randint(0,2**31-2)\n",
    "    zT = prepare_latent_from_seed(seed0, image_size=image_size)  # shape [1,C,H/8,W/8]\n",
    "    # Denoise once to get baseline\n",
    "    with torch.no_grad():\n",
    "        img = pipe(prompt, height=image_size, width=image_size, num_inference_steps=steps, guidance_scale=guidance_scale, latents=zT).images[0]\n",
    "    best_img = img\n",
    "    best_score = clip_score(prompt, img)\n",
    "    print(f'Initial seed {seed0}, CLIP {best_score:.4f}')\n",
    "    for it in range(num_iters):\n",
    "        # compute gamma: paper uses gamma = 1 - sqrt(s) where s is VQA score; here use CLIP proxy in [0,1] after normalization.\n",
    "        if gamma_rule == 'score':\n",
    "            gamma = 1.0 - math.sqrt(max(0.0, min(1.0, (best_score + 1)/2)))  # crude mapping CLIP[-1,1] -> [0,1]\n",
    "        else:\n",
    "            gamma = 0.2\n",
    "        cand_list = []\n",
    "        t0 = time.time()\n",
    "        for i in range(num_cands):\n",
    "            sigma = torch.randn_like(zT)\n",
    "            zT_cand = math.sqrt(1.0 - gamma) * zT + math.sqrt(gamma) * sigma\n",
    "            with torch.no_grad():\n",
    "                img_c = pipe(prompt, height=image_size, width=image_size, num_inference_steps=steps, guidance_scale=guidance_scale, latents=zT_cand).images[0]\n",
    "            sc = clip_score(prompt, img_c)\n",
    "            cand_list.append((sc, zT_cand, img_c))\n",
    "        sc_chosen, zT_chosen, img_chosen = max(cand_list, key=lambda x: x[0])\n",
    "        if sc_chosen > best_score:\n",
    "            best_score = sc_chosen\n",
    "            zT = zT_chosen\n",
    "            best_img = img_chosen\n",
    "            print(f'Iter {it+1}: improved CLIP {best_score:.4f} (time {time.time()-t0:.1f}s)')\n",
    "        else:\n",
    "            print(f'Iter {it+1}: no improvement (best {best_score:.4f}) (time {time.time()-t0:.1f}s)')\n",
    "    return best_img, best_score\n",
    "\n",
    "# Small test (use small num_iters/cands first)\n",
    "prompt = 'A photo of a small brown dog running on grass'\n",
    "img, sc = noise_diffusion_latent(prompt, num_iters=1, num_cands=2)\n",
    "display(img)\n",
    "print('Final CLIP', sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844cbe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save last generated image (if any) to drive/workspace\n",
    "out_path = '/content/noise_diffusion_result.png'\n",
    "try:\n",
    "    img.save(out_path)\n",
    "    print('Saved to', out_path)\n",
    "except Exception as e:\n",
    "    print('No img object found in this cell scope; create image first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a721a",
   "metadata": {},
   "source": [
    "---\n",
    "## Next steps / tips\n",
    "- **Pilot:** run CLIP-MonteCarlo on 100 prompts to choose hyperparams (num_iters, num_cands, steps).  \n",
    "- **Scale:** after pilot, run larger batches and log results (CLIPScore, LPIPS).  \n",
    "- **Reproducibility:** save seeds & latents for top results.  \n",
    "- **If you have large GPU (>=48GB) available later:** increase image size to 512, steps to 50 and num_cands/iters for full reproduction.\n",
    "\n",
    "If you'd like, I can:\n",
    "- produce a version that logs metrics to CSV,\n",
    "- split prompts into shards for distributed runs,\n",
    "- or generate a downloadable .ipynb file for direct upload to Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34bd096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install logging helpers (run once)\n",
    "import sys\n",
    "!pip install -q pandas tqdm\n",
    "print('pandas and tqdm available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465c9b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide a list of prompts to process in batch.\n",
    "# You can either edit the 'prompts' list below directly or upload a .txt file with one prompt per line.\n",
    "from google.colab import files\n",
    "import os, csv, time\n",
    "\n",
    "# Example prompts (replace or extend)\n",
    "prompts = [\n",
    "    \"A photorealistic red vintage car parked in front of a Victorian house\",\n",
    "    \"A cute golden retriever puppy playing with a blue ball on grass\",\n",
    "    \"A modern kitchen interior with wooden cabinets and a marble island\",\n",
    "]\n",
    "\n",
    "# Optionally, upload a text file with prompts (one per line)\n",
    "print(\"Current num prompts:\", len(prompts))\n",
    "uploaded = files.upload(button_label='Upload prompt file (optional, one prompt per line)')\n",
    "if uploaded:\n",
    "    # take the first uploaded file\n",
    "    fname = next(iter(uploaded.keys()))\n",
    "    with open(fname, 'r', encoding='utf-8') as f:\n",
    "        lines = [l.strip() for l in f.readlines() if l.strip()]\n",
    "    if lines:\n",
    "        prompts = lines\n",
    "    print('Loaded', len(prompts), 'prompts from', fname)\n",
    "\n",
    "# Save prompts to disk for reproducibility\n",
    "with open('/content/prompts_list.txt', 'w', encoding='utf-8') as f:\n",
    "    for p in prompts:\n",
    "        f.write(p + '\\n')\n",
    "print('Prompts saved to /content/prompts_list.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f42351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prompts from repo datasets JSON files (if present)\n",
    "import os, json, glob\n",
    "prompts = []\n",
    "\n",
    "repo_datasets_dir = '/content/NoiseDiffusion/datasets'\n",
    "if not os.path.exists(repo_datasets_dir):\n",
    "    repo_datasets_dir = '/mnt/data/NoiseDiffusion/datasets'  # alternate location if notebook ran elsewhere\n",
    "\n",
    "print('Looking for JSON files in', repo_datasets_dir)\n",
    "if os.path.exists(repo_datasets_dir):\n",
    "    json_files = sorted(glob.glob(os.path.join(repo_datasets_dir, '*.json')))\n",
    "    print('Found', len(json_files), 'json file(s)')\n",
    "    for jf in json_files:\n",
    "        try:\n",
    "            with open(jf, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            # Robust extraction: if data is list -> assume list of prompts or dicts\n",
    "            if isinstance(data, list):\n",
    "                # if list of dicts with 'prompt' or 'text' keys, extract those; else if list of strings, extend directly\n",
    "                if len(data) > 0 and isinstance(data[0], dict):\n",
    "                    for item in data:\n",
    "                        if isinstance(item, dict):\n",
    "                            if 'prompt' in item:\n",
    "                                prompts.append(item['prompt'])\n",
    "                            elif 'text' in item:\n",
    "                                prompts.append(item['text'])\n",
    "                            elif 'caption' in item:\n",
    "                                prompts.append(item['caption'])\n",
    "                            else:\n",
    "                                # fallback: try join values\n",
    "                                prompts.append(' '.join(str(v) for v in item.values()))\n",
    "                else:\n",
    "                    # list of strings\n",
    "                    prompts.extend([str(x) for x in data])\n",
    "            elif isinstance(data, dict):\n",
    "                # try common keys\n",
    "                if 'prompts' in data and isinstance(data['prompts'], list):\n",
    "                    prompts.extend([str(x) for x in data['prompts']])\n",
    "                elif 'captions' in data and isinstance(data['captions'], list):\n",
    "                    prompts.extend([str(x) for x in data['captions']])\n",
    "                elif 'annotations' in data and isinstance(data['annotations'], list):\n",
    "                    # COCO-style annotations might be list of dicts with 'caption' or 'text'\n",
    "                    for a in data['annotations']:\n",
    "                        if isinstance(a, dict):\n",
    "                            if 'caption' in a:\n",
    "                                prompts.append(a['caption'])\n",
    "                            elif 'text' in a:\n",
    "                                prompts.append(a['text'])\n",
    "                else:\n",
    "                    # fallback: collect string values\n",
    "                    for k,v in data.items():\n",
    "                        if isinstance(v, str):\n",
    "                            prompts.append(v)\n",
    "                        elif isinstance(v, list):\n",
    "                            prompts.extend([str(x) for x in v if isinstance(x, str)])\n",
    "        except Exception as e:\n",
    "            print('Failed to load', jf, '->', e)\n",
    "\n",
    "# dedupe and clean\n",
    "prompts = [p.strip() for p in prompts if isinstance(p, str) and p.strip()]\n",
    "prompts = list(dict.fromkeys(prompts))  # preserve order, dedupe\n",
    "print('Total prompts extracted:', len(prompts))\n",
    "\n",
    "# If none found, fall back to sample prompts for quick demo\n",
    "if len(prompts) == 0:\n",
    "    print('No prompts found in repo datasets. Using default demo prompts.')\n",
    "    prompts = [\n",
    "        'A photorealistic red vintage car parked in front of a Victorian house',\n",
    "        'A cute golden retriever puppy playing with a blue ball on grass',\n",
    "        'A modern kitchen interior with wooden cabinets and a marble island',\n",
    "    ]\n",
    "\n",
    "# Save prompts to disk for reproducibility\n",
    "out_file = '/content/prompts_list.txt'\n",
    "with open(out_file, 'w', encoding='utf-8') as f:\n",
    "    for p in prompts:\n",
    "        f.write(p + '\\n')\n",
    "print('Saved prompts to', out_file)\n",
    "print('First 10 prompts:')\n",
    "for i,p in enumerate(prompts[:10]):\n",
    "    print(i+1, p)\n",
    "\n",
    "# expose 'prompts' in notebook globals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4aa9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch runner for CLIP-MonteCarlo (fast proxy). Logs metrics to CSV and saves images.\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os, time, csv\n",
    "out_dir = '/content/noise_diffusion_outputs'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "csv_path = os.path.join(out_dir, 'results_clip_mc.csv')\n",
    "\n",
    "# Parameters (tune as needed)\n",
    "ITERATIONS = DEFAULT_NUM_ITERS if 'DEFAULT_NUM_ITERS' in globals() else 2\n",
    "CANDIDATES = DEFAULT_NUM_CANDS if 'DEFAULT_NUM_CANDS' in globals() else 4\n",
    "STEPS = DEFAULT_STEPS if 'DEFAULT_STEPS' in globals() else 20\n",
    "IMAGE_SIZE = DEFAULT_IMAGE_SIZE if 'DEFAULT_IMAGE_SIZE' in globals() else 384\n",
    "GUIDANCE = 7.5\n",
    "\n",
    "results = []\n",
    "# If CSV exists, resume from existing\n",
    "if os.path.exists(csv_path):\n",
    "    df_prev = pd.read_csv(csv_path)\n",
    "    done_prompts = set(df_prev['prompt'].tolist())\n",
    "    print('Resuming. Already processed', len(done_prompts), 'prompts.')\n",
    "else:\n",
    "    done_prompts = set()\n",
    "\n",
    "for idx, prompt in enumerate(tqdm(prompts, desc='Prompts')):\n",
    "    if prompt in done_prompts:\n",
    "        continue\n",
    "    t0 = time.time()\n",
    "    img, seed, score = clip_monte_carlo(prompt, iterations=ITERATIONS, candidates=CANDIDATES, steps=STEPS, image_size=IMAGE_SIZE)\n",
    "    elapsed = time.time() - t0\n",
    "    fname = f'clipmc_{idx:04d}_seed{seed}.png'\n",
    "    img.save(os.path.join(out_dir, fname))\n",
    "    row = {'idx': idx, 'prompt': prompt, 'method': 'clip_mc', 'seed': seed, 'score': float(score), 'image_path': fname, 'time_s': elapsed}\n",
    "    results.append(row)\n",
    "    # Append to CSV after each prompt for safety\n",
    "    df = pd.DataFrame(results)\n",
    "    if os.path.exists(csv_path):\n",
    "        df_prev = pd.read_csv(csv_path)\n",
    "        df = pd.concat([df_prev, df], ignore_index=True)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    results = []  # reset buffer for incremental writes\n",
    "print('Batch CLIP-MC completed (or resumed). Results saved to', csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eae94fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch runner for NoiseDiffusion latent update (more expensive). Logs to CSV and saves images.\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os, time, math\n",
    "out_dir = '/content/noise_diffusion_outputs'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "csv_path2 = os.path.join(out_dir, 'results_noise_diffusion.csv')\n",
    "\n",
    "# Parameters (tune conservatively)\n",
    "NUM_ITERS = DEFAULT_NUM_ITERS if 'DEFAULT_NUM_ITERS' in globals() else 2\n",
    "NUM_CANDS = DEFAULT_NUM_CANDS if 'DEFAULT_NUM_CANDS' in globals() else 4\n",
    "STEPS = DEFAULT_STEPS if 'DEFAULT_STEPS' in globals() else 20\n",
    "IMAGE_SIZE = DEFAULT_IMAGE_SIZE if 'DEFAULT_IMAGE_SIZE' in globals() else 384\n",
    "GUIDANCE = 7.5\n",
    "\n",
    "results = []\n",
    "if os.path.exists(csv_path2):\n",
    "    df_prev = pd.read_csv(csv_path2)\n",
    "    done_prompts = set(df_prev['prompt'].tolist())\n",
    "    print('Resuming NoiseDiffusion. Already processed', len(done_prompts), 'prompts.')\n",
    "else:\n",
    "    done_prompts = set()\n",
    "\n",
    "for idx, prompt in enumerate(tqdm(prompts, desc='Prompts ND')):\n",
    "    if prompt in done_prompts:\n",
    "        continue\n",
    "    t0 = time.time()\n",
    "    # run noise diffusion latent update with conservative params\n",
    "    img, score = noise_diffusion_latent(prompt, num_iters=NUM_ITERS, num_cands=NUM_CANDS, steps=STEPS, image_size=IMAGE_SIZE, guidance_scale=GUIDANCE, gamma_rule='score')\n",
    "    elapsed = time.time() - t0\n",
    "    fname = f'nd_{idx:04d}.png'\n",
    "    img.save(os.path.join(out_dir, fname))\n",
    "    row = {'idx': idx, 'prompt': prompt, 'method': 'noise_diffusion', 'score': float(score), 'image_path': fname, 'time_s': elapsed, 'num_iters': NUM_ITERS, 'num_cands': NUM_CANDS}\n",
    "    results.append(row)\n",
    "    # write incrementally\n",
    "    df = pd.DataFrame(results)\n",
    "    if os.path.exists(csv_path2):\n",
    "        df_prev = pd.read_csv(csv_path2)\n",
    "        df = pd.concat([df_prev, df], ignore_index=True)\n",
    "    df.to_csv(csv_path2, index=False)\n",
    "    results = []\n",
    "print('Batch NoiseDiffusion completed (or resumed). Results saved to', csv_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6746db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List output files and provide download links for convenience\n",
    "import os, glob, pandas as pd, IPython.display as disp\n",
    "out_dir = '/content/noise_diffusion_outputs'\n",
    "print('Output dir:', out_dir)\n",
    "for f in sorted(glob.glob(out_dir + '/*'))[:50]:\n",
    "    print(f)\n",
    "# show CSV heads if exist\n",
    "for csvf in ['results_clip_mc.csv','results_noise_diffusion.csv']:\n",
    "    p = os.path.join(out_dir, csvf)\n",
    "    if os.path.exists(p):\n",
    "        print('---', csvf, '---')\n",
    "        display(pd.read_csv(p).head())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
